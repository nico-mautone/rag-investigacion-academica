{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Sistema de asistencia en investigación académica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pinecone import Pinecone\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus - ACL Anthology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../acl-publication-info.74k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "      <th>corpus_paper_id</th>\n",
       "      <th>pdf_hash</th>\n",
       "      <th>numcitedby</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>address</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>booktitle</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>pages</th>\n",
       "      <th>doi</th>\n",
       "      <th>number</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>editor</th>\n",
       "      <th>isbn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O02-2002</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>18022704</td>\n",
       "      <td>0b09178ac8d17a92f16140365363d8df88c757d0</td>\n",
       "      <td>14</td>\n",
       "      <td>https://aclanthology.org/O02-2002</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>International Journal of Computational Linguis...</td>\n",
       "      <td>Chen, Keh-Jiann  and\\nYou, Jia-Ming</td>\n",
       "      <td>A Study on Word Similarity using Context Vecto...</td>\n",
       "      <td>37--58</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L02-1310</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8220988</td>\n",
       "      <td>8d5e31610bc82c2abc86bc20ceba684c97e66024</td>\n",
       "      <td>93</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the Third International Confere...</td>\n",
       "      <td>Mihalcea, Rada F.</td>\n",
       "      <td>Bootstrapping Large Sense Tagged Corpora</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R13-1042</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>16703040</td>\n",
       "      <td>3eb736b17a5acb583b9a9bd99837427753632cdb</td>\n",
       "      <td>10</td>\n",
       "      <td>https://aclanthology.org/R13-1042</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the International Conference Re...</td>\n",
       "      <td>Jamison, Emily  and\\nGurevych, Iryna</td>\n",
       "      <td>Headerless, Quoteless, but not Hopeless? Using...</td>\n",
       "      <td>327--335</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W05-0819</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>1215281</td>\n",
       "      <td>b20450f67116e59d1348fc472cfc09f96e348f55</td>\n",
       "      <td>15</td>\n",
       "      <td>https://aclanthology.org/W05-0819</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the {ACL} Workshop on Building ...</td>\n",
       "      <td>Aswani, Niraj  and\\nGaizauskas, Robert</td>\n",
       "      <td>Aligning Words in {E}nglish-{H}indi Parallel C...</td>\n",
       "      <td>115--118</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L02-1309</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>18078432</td>\n",
       "      <td>011e943b64a78dadc3440674419821ee080f0de3</td>\n",
       "      <td>12</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the Third International Confere...</td>\n",
       "      <td>Suyaga, Fumiaki  and\\nTakezawa, Toshiyuki  and...</td>\n",
       "      <td>Proposal of a very-large-corpus acquisition me...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R13-1044</td>\n",
       "      <td>The paper 1 presents a rule-based approach to ...</td>\n",
       "      <td>The paper 1 presents a rule-based approach to ...</td>\n",
       "      <td>2491460</td>\n",
       "      <td>c0f1047fe0f95c367184d494e78bb07b11ee3608</td>\n",
       "      <td>2</td>\n",
       "      <td>https://aclanthology.org/R13-1044</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the International Conference Re...</td>\n",
       "      <td>K{\\k{e}}dzia, Pawe{\\l}  and\\nMaziarz, Marek</td>\n",
       "      <td>Recognizing semantic relations within {P}olish...</td>\n",
       "      <td>342--349</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>W05-0818</td>\n",
       "      <td>In this paper we describe LIHLA, a lexical ali...</td>\n",
       "      <td>In this paper we describe LIHLA, a lexical ali...</td>\n",
       "      <td>15322146</td>\n",
       "      <td>ff3f05120d24e5dac2879f25402993bc6355f780</td>\n",
       "      <td>5</td>\n",
       "      <td>https://aclanthology.org/W05-0818</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the {ACL} Workshop on Building ...</td>\n",
       "      <td>Caseli, Helena M.  and\\nNunes, Maria G. V.  an...</td>\n",
       "      <td>{LIHLA}: Shared Task System Description</td>\n",
       "      <td>111--114</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L02-1313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>649937</td>\n",
       "      <td>c5c1643517ee6646c47b4ee2b8443d4f62ee1ae5</td>\n",
       "      <td>4</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the Third International Confere...</td>\n",
       "      <td>Baldwin, Timothy  and\\nBilac, Slaven  and\\nOku...</td>\n",
       "      <td>Enhanced {J}apanese Electronic Dictionary Look-up</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>R13-1045</td>\n",
       "      <td>We describe an approach to building a morpholo...</td>\n",
       "      <td>We describe an approach to building a morpholo...</td>\n",
       "      <td>690455</td>\n",
       "      <td>0b125557ba23075532380e88fb990933838975b7</td>\n",
       "      <td>2</td>\n",
       "      <td>https://aclanthology.org/R13-1045</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the International Conference Re...</td>\n",
       "      <td>Khaliq, Bilal  and\\nCarroll, John</td>\n",
       "      <td>Unsupervised Induction of {A}rabic Root and Pa...</td>\n",
       "      <td>350--356</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>W05-0821</td>\n",
       "      <td>Statistical machine translation systems use a ...</td>\n",
       "      <td>Statistical machine translation systems use a ...</td>\n",
       "      <td>1966857</td>\n",
       "      <td>2a05c9c5373a3e1e01b8161e6687b960ab3d2ff5</td>\n",
       "      <td>55</td>\n",
       "      <td>https://aclanthology.org/W05-0821</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the {ACL} Workshop on Building ...</td>\n",
       "      <td>Kirchhoff, Katrin  and\\nYang, Mei</td>\n",
       "      <td>Improved Language Modeling for Statistical Mac...</td>\n",
       "      <td>125--128</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acl_id                                           abstract  \\\n",
       "0  O02-2002  There is a need to measure word similarity whe...   \n",
       "1  L02-1310                                                      \n",
       "2  R13-1042  Thread disentanglement is the task of separati...   \n",
       "3  W05-0819  In this paper, we describe a word alignment al...   \n",
       "4  L02-1309                                                      \n",
       "5  R13-1044  The paper 1 presents a rule-based approach to ...   \n",
       "6  W05-0818  In this paper we describe LIHLA, a lexical ali...   \n",
       "7  L02-1313                                                      \n",
       "8  R13-1045  We describe an approach to building a morpholo...   \n",
       "9  W05-0821  Statistical machine translation systems use a ...   \n",
       "\n",
       "                                           full_text  corpus_paper_id  \\\n",
       "0  There is a need to measure word similarity whe...         18022704   \n",
       "1                                                             8220988   \n",
       "2  Thread disentanglement is the task of separati...         16703040   \n",
       "3  In this paper, we describe a word alignment al...          1215281   \n",
       "4                                                            18078432   \n",
       "5  The paper 1 presents a rule-based approach to ...          2491460   \n",
       "6  In this paper we describe LIHLA, a lexical ali...         15322146   \n",
       "7                                                              649937   \n",
       "8  We describe an approach to building a morpholo...           690455   \n",
       "9  Statistical machine translation systems use a ...          1966857   \n",
       "\n",
       "                                   pdf_hash  numcitedby  \\\n",
       "0  0b09178ac8d17a92f16140365363d8df88c757d0          14   \n",
       "1  8d5e31610bc82c2abc86bc20ceba684c97e66024          93   \n",
       "2  3eb736b17a5acb583b9a9bd99837427753632cdb          10   \n",
       "3  b20450f67116e59d1348fc472cfc09f96e348f55          15   \n",
       "4  011e943b64a78dadc3440674419821ee080f0de3          12   \n",
       "5  c0f1047fe0f95c367184d494e78bb07b11ee3608           2   \n",
       "6  ff3f05120d24e5dac2879f25402993bc6355f780           5   \n",
       "7  c5c1643517ee6646c47b4ee2b8443d4f62ee1ae5           4   \n",
       "8  0b125557ba23075532380e88fb990933838975b7           2   \n",
       "9  2a05c9c5373a3e1e01b8161e6687b960ab3d2ff5          55   \n",
       "\n",
       "                                                 url  \\\n",
       "0                  https://aclanthology.org/O02-2002   \n",
       "1  http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "2                  https://aclanthology.org/R13-1042   \n",
       "3                  https://aclanthology.org/W05-0819   \n",
       "4  http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "5                  https://aclanthology.org/R13-1044   \n",
       "6                  https://aclanthology.org/W05-0818   \n",
       "7  http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "8                  https://aclanthology.org/R13-1045   \n",
       "9                  https://aclanthology.org/W05-0821   \n",
       "\n",
       "                                        publisher  \\\n",
       "0                                            None   \n",
       "1  European Language Resources Association (ELRA)   \n",
       "2                   INCOMA Ltd. Shoumen, BULGARIA   \n",
       "3       Association for Computational Linguistics   \n",
       "4  European Language Resources Association (ELRA)   \n",
       "5                   INCOMA Ltd. Shoumen, BULGARIA   \n",
       "6       Association for Computational Linguistics   \n",
       "7  European Language Resources Association (ELRA)   \n",
       "8                   INCOMA Ltd. Shoumen, BULGARIA   \n",
       "9       Association for Computational Linguistics   \n",
       "\n",
       "                              address  year  ...  \\\n",
       "0                                None  2002  ...   \n",
       "1  Las Palmas, Canary Islands - Spain  2002  ...   \n",
       "2                    Hissar, Bulgaria  2013  ...   \n",
       "3                 Ann Arbor, Michigan  2005  ...   \n",
       "4  Las Palmas, Canary Islands - Spain  2002  ...   \n",
       "5                    Hissar, Bulgaria  2013  ...   \n",
       "6                 Ann Arbor, Michigan  2005  ...   \n",
       "7  Las Palmas, Canary Islands - Spain  2002  ...   \n",
       "8                    Hissar, Bulgaria  2013  ...   \n",
       "9                 Ann Arbor, Michigan  2005  ...   \n",
       "\n",
       "                                           booktitle  \\\n",
       "0  International Journal of Computational Linguis...   \n",
       "1  Proceedings of the Third International Confere...   \n",
       "2  Proceedings of the International Conference Re...   \n",
       "3  Proceedings of the {ACL} Workshop on Building ...   \n",
       "4  Proceedings of the Third International Confere...   \n",
       "5  Proceedings of the International Conference Re...   \n",
       "6  Proceedings of the {ACL} Workshop on Building ...   \n",
       "7  Proceedings of the Third International Confere...   \n",
       "8  Proceedings of the International Conference Re...   \n",
       "9  Proceedings of the {ACL} Workshop on Building ...   \n",
       "\n",
       "                                              author  \\\n",
       "0                Chen, Keh-Jiann  and\\nYou, Jia-Ming   \n",
       "1                                  Mihalcea, Rada F.   \n",
       "2               Jamison, Emily  and\\nGurevych, Iryna   \n",
       "3             Aswani, Niraj  and\\nGaizauskas, Robert   \n",
       "4  Suyaga, Fumiaki  and\\nTakezawa, Toshiyuki  and...   \n",
       "5        K{\\k{e}}dzia, Pawe{\\l}  and\\nMaziarz, Marek   \n",
       "6  Caseli, Helena M.  and\\nNunes, Maria G. V.  an...   \n",
       "7  Baldwin, Timothy  and\\nBilac, Slaven  and\\nOku...   \n",
       "8                  Khaliq, Bilal  and\\nCarroll, John   \n",
       "9                  Kirchhoff, Katrin  and\\nYang, Mei   \n",
       "\n",
       "                                               title     pages   doi number  \\\n",
       "0  A Study on Word Similarity using Context Vecto...    37--58  None   None   \n",
       "1           Bootstrapping Large Sense Tagged Corpora      None  None   None   \n",
       "2  Headerless, Quoteless, but not Hopeless? Using...  327--335  None   None   \n",
       "3  Aligning Words in {E}nglish-{H}indi Parallel C...  115--118  None   None   \n",
       "4  Proposal of a very-large-corpus acquisition me...      None  None   None   \n",
       "5  Recognizing semantic relations within {P}olish...  342--349  None   None   \n",
       "6            {LIHLA}: Shared Task System Description  111--114  None   None   \n",
       "7  Enhanced {J}apanese Electronic Dictionary Look-up      None  None   None   \n",
       "8  Unsupervised Induction of {A}rabic Root and Pa...  350--356  None   None   \n",
       "9  Improved Language Modeling for Statistical Mac...  125--128  None   None   \n",
       "\n",
       "  volume journal editor  isbn  \n",
       "0   None    None   None  None  \n",
       "1   None    None   None  None  \n",
       "2   None    None   None  None  \n",
       "3   None    None   None  None  \n",
       "4   None    None   None  None  \n",
       "5   None    None   None  None  \n",
       "6   None    None   None  None  \n",
       "7   None    None   None  None  \n",
       "8   None    None   None  None  \n",
       "9   None    None   None  None  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Get the first 10 rows\n",
    "# df_reduced = df.head(10)\n",
    "# df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Study on Word Similarity using Context Vector Models\n",
      "There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.\n",
      "There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together. Introduction It is well known that word-sense is defined by a word's co-occurrence context. The context vectors of a word are defined as the probabilistic distributions of its left and right co-occurrence contexts. Conventionally , the similarity between two context vectors is measured based on their cosine distance [Alshawi and Cater, 1994; Grishman and Sterling, 1994; Pereira et al., 1993; Ruge, 1992; Salton, 1989] . However, the conventional measurement * Institute of Information Science, Academia Sinica E-mail: kchen@iis.sinica.edu.tw; swimming@hp.iis.sinica.edu.tw suffers from the following drawbacks. First of all, the information in the context vectors is vague. All co-occurrence words are collected without distinguishing whether they are syntactically or semantically related. Second, the coordinates are not pair-wise independent (i.e., the axes are not orthogonal), and it is hard to apply singular value decomposition to find the orthogonal vectors [Schutze, 1992] . In this paper, we propose to use only syntactic related co-occurrences as context vectors [Dekang Lin, 1998 ] and adopt information theoretic models to solve the above problems. In our study, the context vectors of a word are defined as the probabilistic distributions of its thematic roles and left/right co-occurrence semantic classes. The context features are derived from a treebank. All context features are weighted according to their TF × IDF values (the product of the term frequency and inverse document frequency) [Salton, 1989] . For the context features, the Cilin semantic classes (a Chinese thesaurus) are adopted. The Cilin semantic classes are divided into 4 different levels of granularity. In order to cope with the data sparseness problem, the weighted average of the similarity values at four different levels will be the similarity measure of two words. The weight for each level is equal to the information-content of that level [Shannon, 1948; Manning and Schutze 1999] . A agglomerative clustering algorithm is applied to group similar words according to the above defined similarity measure. Obviously, words with similar behavior in the corpus will be grouped together. We have compared the clustering results to the Cilin classifications. It turns out that words in the same synonym class and with the same syntactic categories have higher similarity values than the words with different syntactic categories. Data Resources Ideally, to derive context vectors, a large corpus with semantic tags is required. Furthermore, to extract co-occurrence words along with their exact syntactic and semantic relations, the corpus structure has to be annotated. However, such an ideal corpus does not exist. Therefore, in this paper we will adopt the resources that are available and try to derive a useful but imperfect Chinese tree bank. Since the similarity measure based on the vector space model is a rough estimation, minor errors made at the stage of context vector extraction are acceptable. Sinica Corpus The Sinica corpus contains 12,532 documents and nearly 5 million words. Each sentence in the corpus was parsed by a rule parser [Chen, 1996] . The parsed trees were tagged with the structure brackets, syntactic categories and thematic roles of each constituent [Huang et al., 2000] as exemplified below, (Sinica corpus: http://www.sinica.edu.tw/ftms -bin/kiwi.sh): Original sentence: 小 'small' 狗 'dog' 跳舞 'dance' Parsed tree : S(Agent:NP:(Property:Adj:'小 small'| Head: N: ' 狗 dog' )|Head:V: ' 跳舞 dance') Although these labels may not be exactly correct, we believe that, even with these minor errors, the majority of word -to-word relations extracted from the trees are correct. However, the semantic label is not provided for each word in the parsed trees. In this paper, we will use Cilin classifications for semantic labeling. Cilin-a Chinese Thesaurus Cilin provides the Mandarin synonym sets in a hierarchical structure [Mei et. al., 1984] . It contains 51,708 Chinese words, and 3918 classes. There are five levels in the Cilin semantic hierarchy, denoted in the format L 1 -L 2 -L 3 -L 4 -L 5 . For example, the Cilin class of the word 我們 'we' is \"A -a-02-2-01\". In level 1, \"A\", denotes the semantic class of human; in level 2, \"a\", indicates a group of general terms; level 3, \"02\", means pronouns in the first person, and in level 4, \"2\" represents the plural property. In level 5, \"01\" represents the order rank in the level 4 group. This means that \"01\" in level 5 is the first prototypical concept representation of \"A-a-02-2\". In the rest of this paper, only the first four levels will be used. The fifth level is for sense disambiguation only (section 2.3). Sense Disambiguation A polysemous word has more than one Cilin semantic class. In order to tag appropriate Cilin classes, we have designed a simple sense tagging method as follows [Wilks, 1999] . The sense tagging algorithm is based on the facts that the syntactic categories of each word in the tree bank are assigned uniquely, and that each Cilin class has its own major syntactic category. If a word has multiple Cilin classes, we select the sense class whose major syntactic category is the same as the tagged category of this word. For example , 計畫 \" Jie -Hwa\" has two meanings. One is for \"project\" as a noun and the other is \"attempt\", therefore, if 計畫 \"Jie -Hwa\" was tagged with a noun category, we will assign the Cilin class whose major category is \"project\". Sense ambiguity can be distinguished by measure of syntactic properties for most words. However, there are still cases in which the syntactic category constraints cannot resolve the sense ambiguities. Then, we simply choose the prototypical sense class, i.e, the word that has the highest rank in this sense class with respect to all its sense classes in Cilin. The Extraction of Co-occurrence Data The extracted syntactically related pairs have either a head-modifier relation or head-argument relation. For instance, two syntactically related pairs extracted from the example in section 2.1 are: (<Thematic role > <Cilin> <word1>), (<Thematic role> <Cilin> <word2>) (agent B -i-07-2 狗'dog'), (Head(S) H-h-04-2 跳舞'dance' ) (property E -a-03-3 小'small'), (Head(NP) B -i-07-2 狗'dog') The context data of the word 1 狗 \"dog\" consists of its thematic role \"agent\" and the Cilin class \"B-i-07-2\" ; the word 2 跳舞 'dance' consists the thematic role \"Head(S)\" and the Cilin class \"H-h-04-2\" and so on. The word 小 \"small\" and 跳舞 \"dance\" are not syntactically related even though they co-occur. Therefore , they will not be extracted. Context Vector Model There are three context vectors of a word: role vector, left context vector and right context vector. The role vector is a fixed 48-dimension vector, and each dimension value is equal to the probabilistic distribution of its thematic roles. The left/right context vectors are closer to the probabilistic distributions of its left/right co-occurrence words and their semantic classes. The role vector characterizes a word based more on syntax and less on semantics, but the left/right context vectors are just the opposite. The cosine distance between their context vectors is a measure of the similarity of the two words. We will illustrate the derivations of context vectors and their similarity rating with a simplified example using (貓 \"cat\", 狗 \"dog\"). The role vector of \"dog\" is {127, 207, 169… , 0} 48 , which represents the values of \"agent\", \"goal\", \"theme\"… and \"topic\" respectively, generated by Equation (1). The role vector of \"cat\" is {28, 73, 56… , 0} 48 , which is also acquired by Equation (1). Role vector of word W = { V 1 , V 2 ,… ,V 48 } 48 Vi = Frequency (R i ) × log (1/P i ) (1) R i : We label thematic roles \"agent\", \"goal\"… and \"topic\" from R 1 to R 48 listed in Table2 in the Appendix. Frequency (R i ): The frequency of R i played by word W in the corpus. P i : = Total frequency of R i in the corpus / Total frequency of all roles in the corpus. log(1/P i ): The information-context of R i [Shannon, 1948; Manning and Schutze, 1999] The derivation of left/right context vectors is a bit more complicated. The syntactically related co-occurrence word pairs are derived first as illustrated in section 2.4. We will illustrate the derivation of the left context vector only. The right context vector can be derived simila rly. The left co-occurrence word vector of word W is generated from frequency(word i ), where word i precedes and is syntactically related to W in the corpus. Due to the data sparseness problem, the feature dimensions of context vectors are generalized into C ilin classes instead of co-occurrence words. The generalization process reduces the effect of data sparseness. On the other hand, it also reduces the precision of characterization since each word has different information content and two words that have the same co-occurrence semantic classes may not share the same co-occurrence words. In order to resolve the above dilemma, when we compare the similarity between word X and word Y , 4 levels of left context vectors and right context vectors for word X and word Y are created 1 . The weighting of each feature dimension is adjusted using the TF*IDF value if word X and word Y have shared context words. Equation ( 2 ) illustrates the creation of the 4 th level left context vector of word X . The other context vectors for word X and word Y are created by a similar way. Left context vector of word X = {f1, f2,… ,f 3918 } L4 Where fi = Sum of { TF(word j ) × IDF(word j ) if word X has the same neighbor word j with word Y TF(word j ) if word X does not have the same neighbor word j with word Y } ; for every left co-occurrence word j with the ith Cilin semantic class. (2) TF(word j ): the frequency of pair ( word j , Cilin(word j ) ) in wordx' s co-occurrence context. IDF(word j ): -log(the number of the documents that contains the word j /total document number of the corpus) In Equation ( 2 ), we adjust the term weight using TF × IDF, which is commonly used in the field of information retrieval [Salton, 1989] to adjust the discrimination power of each feature dimension. We will examine the difference in the adjustment of weights using TF × IDF and TF in section 4.2. We will next give a simplified example. Assume that the word 狗 \"dog\" has only three left syntactically related words: ( 小 \"small\" Ea033 ) with frequency 30, ( 可愛 \"cute\" Ed401 ) with frequency 5 and ( 養 \"raise\" Ib011 ) with frequency 10; and assume that the word 貓 \"cat\" has only two left syntactically related words: ( 黑 \"black\" Ec043 ) and ( 養 \"raise\" Ib011 ). Assume that we are measuring the similarity between 狗 \"dog\" and 貓 \"cat\". Then, we can compute the left context data of 狗 \"dog\" as {TF(Aa011),… ,TF(Ea033),… ,TF(Ed041),… ,TF(Ib011) × IDF(養'raise') 2 , … , TF(La064) } L4 3 1 IDF(養'raise') = 4.188, the IDF values of all words range from 0.19 to 9.12. 2 The granularities of the 4 levels of semantic classes are partially shown in Figure2. The four left context vectors and their dimensions are shown below and the right context vectors are similarly derived. <LeftCilin1> L1 A vector of 12 dimensions from \"A\" to \"L\". <LeftCilin2> L2 A vector of 94 dimensions from \"Aa\" to \"La\". <LeftCilin3> L3 A vector of 1428 dimensions from \"Aa01\" to \"La06\". <LeftCilin4> L4 A vector of 3918 dimensions from \"Aa011\" to \"La064\". Similarities between Two Context Vectors Once we know the feature vectors of these two words, we can calculate the cosine distance of two vectors as shown in Equation (3). vector A= <a1,a2… ,an>,vector B= <b1,b2… ,bn> ... ) , cos( 1 2 1 2 1 ∑ ∑ ∑ = = = × × = n i n i n i bi ai bi ai B A (3) Therefore, the similarity of the two words x and y can be calculated as the linear combination of the cosine distances of all the feature vectors as shown in the Equation 4 . The weight of each feature vector can be adjusted according to different requirements. For instance, if the syntactic similarity is more important, we can increase the weight w. On the other hand, if the semantic similarity is more important, the weights w1 to w4 can be increased. If more training data is available, the level 4 vector will be more reliable. Hence, the weight w4 should increase. (4) y)} > 4 RightCilin < x, > 4 RightCilin cos(< w42 + y) > LeftCilin4 < x, > LeftCilin4 (< cos w41 { w4 y)} > 3 RightCilin < x, > 3 RightCilin (< cos w32 + y) > LeftCilin3 < x, > LeftCilin3 (< cos w31 { w3 y)} > 2 RightCilin < x, > 2 RightCilin (< cos w22 + y) > LeftCilin2 < x, > LeftCilin2 (< cos w21 { 2 y)} > 1 RightCilin < x, > 1 RightCilin (< cos w12 + y) > LeftCilin1 < , x > LeftCilin1 (< cos w11 { 1 y) > vector role < x, > vector role (< cos w = y) (x, similarity × + × + × + × + × w w In the experiments, w = 0.3, w1 = 0.1 × 0.7, w2 = 0.1 × 0.7, w3 = 0.4 × 0.7, and w4 = 0.4 × 0.7. Similarity Clustering Because of the lack of objective standards for evaluating of similarity measures, a agglomerative clustering algorithm is applied to group similar words according to a similarity value. It turns out that words with similar syntactic usage and similar semantic classes are grouped together. We will evaluate our algorithm by comparing the automatic clustering results with manual classifications of Cilin. Clustering Algorithm To evaluate the proposed similarity measure, we tried to group words according to various parameters. We adopted bottom-up agglomerative clustering algorithm to group words. In order to compare the clustered results with Cilin classifications and reduce the data sparseness, we picked the 1000 highest frequency words in Cilin for testing. First of all, we produced a 1000 × 1000 symmetric similarity matrix called SMatrix, where SMatrix (x, y) = similarity 4 3 2 1 4 , 3 , 2 , 1 K RightCilin K n RighttCili K LeftCilin x K LeftCilin K RightCilin x K RightCilin K RightCilin K n RighttCili K LeftCilin x K LeftCilin K LeftCilin x K LeftCilin 2 1 > < = + + + + = > < + > < + > < + > < > < + > < = > < + > < + > < + > < > < + > < = [i] SMatrix[x] , q , p 1 q p 1 q ≤ ≤ ≤ ≤ < < × = ∑∑ = = p n m m p n n q 1 word contains ) wordj ( Group m 1 word )contains wordx ( Group x j 0 ) word , (word similarity [x] SMatrix[j] , q , p 1 q p 1 q ≤ ≤ ≤ ≤ < < × = ∑∑ = = p n m m p n Clustering Results vs Cilin Classification We will make a comparison between the clustering results and Cilin classifications. There are two simple examples shown in Figure 1 Among our 1000 testing words, the number of words that were clustered in the 4 th level of Cilin was 658; i.e., they were labeled with 459 different level-4 Cilin classes and among them, 342 classes contained only one testing word, and the classes with multiple testing words contained a total of 658 testing words. With the threshold=0.7, our method clustered 830 words, and only 167 words of them were clustered in the correct Cilin class. Therefore, by Equation ( 5 ), recall 4 = 167/658 = 0.25 and with Equation ( 6 ) precision 4 = 167/830 = 0.20. We adopted two methods for measuring similarity; one used Equation TF × IDF, and the other used Equation TF. The results are shown in Figure 3 to Figure 6 in the Appendix. We measured the performance by computing the F -score, which is (recall+precision)/2. We discovered that the best F-score of level1 was that 0.7648 located at a threshold equal to 0.65, the best F-score of level2 was that 0.5178 located at a threshold equal to 0.7, the best F-score of level3 was that 0.3165 located at a threshold equal to 0.8, and that the best F-score of level4 was that 0.2476 located at a the threshold equal to 0.8. All were obtained using TF × IDF strategy. Hence, we can see that the TF × IDF equation achieves better performance than the TF equation does. We list the detailed F-socre data for various parameters in appendix. Although the clustering results didn't fit Cilin completely, they are still alike to some degree. From the results, we find that they are similar to syntax taxonomy under a lower threshold and close to semantic taxonomy under higher thresholds. Cilin classifications re -examined To examine the practicability of our proposed method, we also inspected the similarity values of these 658 testing words which were clustered into 117 4 th level Cilin classes. For each semantic class, the average similarity between words in the class and their standard deviation was comp uted. The results are listed in Table1 in the Appendix. We expected that synonyms would have high similarity values, but this was not always the case. According to the assumption noted above, synonyms might have similar syntactic and semantic contexts in language use. Therefore, the average similarity should be pretty high, and the standard deviation should be quite low. However, some of the results didn't follow the assumption. We analyzed the data offer explanations in the following. The context s of the noun (思想, \"thinking\") and the verbs (考慮,考量,思考, \"think\", \"consider\", \"deliberate\") were quite different. As a result, the average similarity value was quite low, and the standard deviation was very high. A fter we remo ved the noun from the word-set, we recomputed the values and obtained the The results conform to our assumption. They also reveal that the context of synonyms may vary from POS to POS. b) Error in Cilin Classification: The classifications in Cilin could be arbitrary. For example, the three words, 數量 \"quantity\", 多少 \"how many\" and 人數 \"the number of people\", were classified in a Cilin group. They might be slightly related, but grouping them together seems inappropriate according to the following table : Word set Average similarity Stand deviation 數量,多少 人數 0.379825 0.253895 c) Different uses: Differences in their usage cause synonyms to behave differently . For example, when we measured the similarity of 美國 \"America\" to 日本 \"Japan\" and to 中國 \"China\", the results we obtained were 0.86 and 0.62,respectively, for each pair. Accroding to human intuition, they simply refer to names of countries and should not have such different similarity values. The reason for these result is that the corpus we adopted is an original Taiwan corpus. As a result, the usage of 中國 \"China\" is different from that of 美國 \"America\" and 日本 \"Japan\". d) Polysemy: The word senses that Cilin adopted were not those frequently used in the corpus. See the following table : . Word set Average similarity Stand deviation 十分,非常, 特別 0.45054 0.305209 Although the three words, 十分 \"very/ten points\", 非常 \"very\" and 特 別\"special, extraordinary\" might seem to be very close in meaning to \"very\", the polysemous word 特別 \"special, extraordinary\" is different in its major sense. This influenced the result. e) Words with similar contexts might not be synonyms: A disadvantage does exist when the context vector model is used. Words that are similar in terms of their context s might not be similar in meaning. For example, the similarity value of 結婚 \"marry\" and 長大 \"grow\" is 0.8139. Although the two words have similar contexts, they are not alike in meaning. Therefore, the vector space model should incorporate the taxonomy approach to solve this phenomenon. Conclusions In this paper, we have adopted the context vector model to measure word similarity. The following new features have been proposed to enhance the context vector models : a) The weights of features are adjusted by applying TF × IDF. b) Feature vectors are smoothed by using Cilin categories to reduce data sparseness. c) Syntactic and semantic similarity is balanced by using related syntactic contexts only. The performance of our method might have been influenced by the small scale of the Chinese corpus and accuracy of the extracted relations. Further more, Cilin was published a long time ago and has not been update recently, which may have influenced our results. However, our experimental results are encouraging. They supports the theory that using context vectors to measure similarity is feasible and worthy of further research. Appendix\n"
     ]
    }
   ],
   "source": [
    "# print title, abstract and full text of the first row\n",
    "print(df.iloc[0]['title'])\n",
    "print(df.iloc[0]['abstract'])\n",
    "print(df.iloc[0]['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a serverless index\n",
    "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "\n",
    "# pc.create_index(name=\"example-index\", dimension=1024, \n",
    "#     spec=ServerlessSpec(cloud='aws', region='us-east-1') \n",
    "# )\n",
    "\n",
    "# Target the index\n",
    "pinecone_index = pc.Index(\"multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading reduced dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "      <th>corpus_paper_id</th>\n",
       "      <th>pdf_hash</th>\n",
       "      <th>numcitedby</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>address</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>booktitle</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>pages</th>\n",
       "      <th>doi</th>\n",
       "      <th>number</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>editor</th>\n",
       "      <th>isbn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O02-2002</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>18022704</td>\n",
       "      <td>0b09178ac8d17a92f16140365363d8df88c757d0</td>\n",
       "      <td>14</td>\n",
       "      <td>https://aclanthology.org/O02-2002</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>International Journal of Computational Linguis...</td>\n",
       "      <td>Chen, Keh-Jiann  and\\nYou, Jia-Ming</td>\n",
       "      <td>A Study on Word Similarity using Context Vecto...</td>\n",
       "      <td>37--58</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L02-1310</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8220988</td>\n",
       "      <td>8d5e31610bc82c2abc86bc20ceba684c97e66024</td>\n",
       "      <td>93</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the Third International Confere...</td>\n",
       "      <td>Mihalcea, Rada F.</td>\n",
       "      <td>Bootstrapping Large Sense Tagged Corpora</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R13-1042</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>16703040</td>\n",
       "      <td>3eb736b17a5acb583b9a9bd99837427753632cdb</td>\n",
       "      <td>10</td>\n",
       "      <td>https://aclanthology.org/R13-1042</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the International Conference Re...</td>\n",
       "      <td>Jamison, Emily  and\\nGurevych, Iryna</td>\n",
       "      <td>Headerless, Quoteless, but not Hopeless? Using...</td>\n",
       "      <td>327--335</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W05-0819</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>1215281</td>\n",
       "      <td>b20450f67116e59d1348fc472cfc09f96e348f55</td>\n",
       "      <td>15</td>\n",
       "      <td>https://aclanthology.org/W05-0819</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the {ACL} Workshop on Building ...</td>\n",
       "      <td>Aswani, Niraj  and\\nGaizauskas, Robert</td>\n",
       "      <td>Aligning Words in {E}nglish-{H}indi Parallel C...</td>\n",
       "      <td>115--118</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L02-1309</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>18078432</td>\n",
       "      <td>011e943b64a78dadc3440674419821ee080f0de3</td>\n",
       "      <td>12</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the Third International Confere...</td>\n",
       "      <td>Suyaga, Fumiaki  and\\nTakezawa, Toshiyuki  and...</td>\n",
       "      <td>Proposal of a very-large-corpus acquisition me...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36637</th>\n",
       "      <td>P09-1048</td>\n",
       "      <td>Cross-lingual tasks are especially difficult d...</td>\n",
       "      <td>Cross-lingual tasks are especially difficult d...</td>\n",
       "      <td>5730661</td>\n",
       "      <td>5e8a3e6a8e12fa551273e35ea80af630f660af43</td>\n",
       "      <td>32</td>\n",
       "      <td>https://aclanthology.org/P09-1048</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Suntec, Singapore</td>\n",
       "      <td>2009</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the Joint Conference of the 47t...</td>\n",
       "      <td>Parton, Kristen  and\\nMcKeown, Kathleen R.  an...</td>\n",
       "      <td>Who, What, When, Where, Why? Comparing Multipl...</td>\n",
       "      <td>423--431</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36638</th>\n",
       "      <td>Y07-1031</td>\n",
       "      <td>Named Entity Recognition (NER) is always limit...</td>\n",
       "      <td>Named Entity Recognition (NER) is always limit...</td>\n",
       "      <td>13203069</td>\n",
       "      <td>8d8155a1623fccd3eaeafd972a8b9a20080b81c8</td>\n",
       "      <td>11</td>\n",
       "      <td>https://aclanthology.org/Y07-1031</td>\n",
       "      <td>The Korean Society for Language and Informatio...</td>\n",
       "      <td>Seoul National University, Seoul, Korea</td>\n",
       "      <td>2007</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the 21st Pacific Asia Conferenc...</td>\n",
       "      <td>Mao, Xinnian  and\\nXu, Wei  and\\nDong, Yuan  a...</td>\n",
       "      <td>Using Non-Local Features to Improve Named Enti...</td>\n",
       "      <td>303--310</td>\n",
       "      <td>http://hdl.handle.net/2065/29132</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36639</th>\n",
       "      <td>W19-5913</td>\n",
       "      <td>While there has been much work in the language...</td>\n",
       "      <td>While there has been much work in the language...</td>\n",
       "      <td>204827576</td>\n",
       "      <td>22320a1a052dc9a290889ca7089030b6c0b92114</td>\n",
       "      <td>3</td>\n",
       "      <td>https://aclanthology.org/W19-5913</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Stockholm, Sweden</td>\n",
       "      <td>2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the 20th Annual SIGdial Meeting...</td>\n",
       "      <td>Ramanarayanan, Vikram  and\\nMulholland, Matthe...</td>\n",
       "      <td>Scoring Interactional Aspects of Human-Machine...</td>\n",
       "      <td>103--109</td>\n",
       "      <td>10.18653/v1/W19-5913</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36640</th>\n",
       "      <td>1996.amta-1.33</td>\n",
       "      <td></td>\n",
       "      <td>System category: System developed for the Tran...</td>\n",
       "      <td>220270362</td>\n",
       "      <td>b23ed352e5365d5ffc8f3597275c109fa5546a9d</td>\n",
       "      <td>0</td>\n",
       "      <td>https://aclanthology.org/1996.amta-1.33</td>\n",
       "      <td>None</td>\n",
       "      <td>Montreal, Canada</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>Conference of the Association for Machine Tran...</td>\n",
       "      <td>Chandioux, John  and\\nGrimaila, Annette</td>\n",
       "      <td>{LEXIUM}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36641</th>\n",
       "      <td>2022.findings-acl.68</td>\n",
       "      <td>Discourse analysis allows us to attain inferen...</td>\n",
       "      <td>Discourse analysis allows us to attain inferen...</td>\n",
       "      <td>247596807</td>\n",
       "      <td>880aa52dfb3ae7778be7db43727b657dff7eb29e</td>\n",
       "      <td>0</td>\n",
       "      <td>https://aclanthology.org/2022.findings-acl.68</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>Findings of the Association for Computational ...</td>\n",
       "      <td>Atwell, Katherine  and\\nSicilia, Anthony  and\\...</td>\n",
       "      <td>The Change that Matters in Discourse Parsing: ...</td>\n",
       "      <td>824--845</td>\n",
       "      <td>10.18653/v1/2022.findings-acl.68</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36642 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     acl_id  \\\n",
       "0                  O02-2002   \n",
       "1                  L02-1310   \n",
       "2                  R13-1042   \n",
       "3                  W05-0819   \n",
       "4                  L02-1309   \n",
       "...                     ...   \n",
       "36637              P09-1048   \n",
       "36638              Y07-1031   \n",
       "36639              W19-5913   \n",
       "36640        1996.amta-1.33   \n",
       "36641  2022.findings-acl.68   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      There is a need to measure word similarity whe...   \n",
       "1                                                          \n",
       "2      Thread disentanglement is the task of separati...   \n",
       "3      In this paper, we describe a word alignment al...   \n",
       "4                                                          \n",
       "...                                                  ...   \n",
       "36637  Cross-lingual tasks are especially difficult d...   \n",
       "36638  Named Entity Recognition (NER) is always limit...   \n",
       "36639  While there has been much work in the language...   \n",
       "36640                                                      \n",
       "36641  Discourse analysis allows us to attain inferen...   \n",
       "\n",
       "                                               full_text  corpus_paper_id  \\\n",
       "0      There is a need to measure word similarity whe...         18022704   \n",
       "1                                                                 8220988   \n",
       "2      Thread disentanglement is the task of separati...         16703040   \n",
       "3      In this paper, we describe a word alignment al...          1215281   \n",
       "4                                                                18078432   \n",
       "...                                                  ...              ...   \n",
       "36637  Cross-lingual tasks are especially difficult d...          5730661   \n",
       "36638  Named Entity Recognition (NER) is always limit...         13203069   \n",
       "36639  While there has been much work in the language...        204827576   \n",
       "36640  System category: System developed for the Tran...        220270362   \n",
       "36641  Discourse analysis allows us to attain inferen...        247596807   \n",
       "\n",
       "                                       pdf_hash  numcitedby  \\\n",
       "0      0b09178ac8d17a92f16140365363d8df88c757d0          14   \n",
       "1      8d5e31610bc82c2abc86bc20ceba684c97e66024          93   \n",
       "2      3eb736b17a5acb583b9a9bd99837427753632cdb          10   \n",
       "3      b20450f67116e59d1348fc472cfc09f96e348f55          15   \n",
       "4      011e943b64a78dadc3440674419821ee080f0de3          12   \n",
       "...                                         ...         ...   \n",
       "36637  5e8a3e6a8e12fa551273e35ea80af630f660af43          32   \n",
       "36638  8d8155a1623fccd3eaeafd972a8b9a20080b81c8          11   \n",
       "36639  22320a1a052dc9a290889ca7089030b6c0b92114           3   \n",
       "36640  b23ed352e5365d5ffc8f3597275c109fa5546a9d           0   \n",
       "36641  880aa52dfb3ae7778be7db43727b657dff7eb29e           0   \n",
       "\n",
       "                                                     url  \\\n",
       "0                      https://aclanthology.org/O02-2002   \n",
       "1      http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "2                      https://aclanthology.org/R13-1042   \n",
       "3                      https://aclanthology.org/W05-0819   \n",
       "4      http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "...                                                  ...   \n",
       "36637                  https://aclanthology.org/P09-1048   \n",
       "36638                  https://aclanthology.org/Y07-1031   \n",
       "36639                  https://aclanthology.org/W19-5913   \n",
       "36640            https://aclanthology.org/1996.amta-1.33   \n",
       "36641      https://aclanthology.org/2022.findings-acl.68   \n",
       "\n",
       "                                               publisher  \\\n",
       "0                                                   None   \n",
       "1         European Language Resources Association (ELRA)   \n",
       "2                          INCOMA Ltd. Shoumen, BULGARIA   \n",
       "3              Association for Computational Linguistics   \n",
       "4         European Language Resources Association (ELRA)   \n",
       "...                                                  ...   \n",
       "36637          Association for Computational Linguistics   \n",
       "36638  The Korean Society for Language and Informatio...   \n",
       "36639          Association for Computational Linguistics   \n",
       "36640                                               None   \n",
       "36641          Association for Computational Linguistics   \n",
       "\n",
       "                                       address  year  ...  \\\n",
       "0                                         None  2002  ...   \n",
       "1           Las Palmas, Canary Islands - Spain  2002  ...   \n",
       "2                             Hissar, Bulgaria  2013  ...   \n",
       "3                          Ann Arbor, Michigan  2005  ...   \n",
       "4           Las Palmas, Canary Islands - Spain  2002  ...   \n",
       "...                                        ...   ...  ...   \n",
       "36637                        Suntec, Singapore  2009  ...   \n",
       "36638  Seoul National University, Seoul, Korea  2007  ...   \n",
       "36639                        Stockholm, Sweden  2019  ...   \n",
       "36640                         Montreal, Canada  1996  ...   \n",
       "36641                          Dublin, Ireland  2022  ...   \n",
       "\n",
       "                                               booktitle  \\\n",
       "0      International Journal of Computational Linguis...   \n",
       "1      Proceedings of the Third International Confere...   \n",
       "2      Proceedings of the International Conference Re...   \n",
       "3      Proceedings of the {ACL} Workshop on Building ...   \n",
       "4      Proceedings of the Third International Confere...   \n",
       "...                                                  ...   \n",
       "36637  Proceedings of the Joint Conference of the 47t...   \n",
       "36638  Proceedings of the 21st Pacific Asia Conferenc...   \n",
       "36639  Proceedings of the 20th Annual SIGdial Meeting...   \n",
       "36640  Conference of the Association for Machine Tran...   \n",
       "36641  Findings of the Association for Computational ...   \n",
       "\n",
       "                                                  author  \\\n",
       "0                    Chen, Keh-Jiann  and\\nYou, Jia-Ming   \n",
       "1                                      Mihalcea, Rada F.   \n",
       "2                   Jamison, Emily  and\\nGurevych, Iryna   \n",
       "3                 Aswani, Niraj  and\\nGaizauskas, Robert   \n",
       "4      Suyaga, Fumiaki  and\\nTakezawa, Toshiyuki  and...   \n",
       "...                                                  ...   \n",
       "36637  Parton, Kristen  and\\nMcKeown, Kathleen R.  an...   \n",
       "36638  Mao, Xinnian  and\\nXu, Wei  and\\nDong, Yuan  a...   \n",
       "36639  Ramanarayanan, Vikram  and\\nMulholland, Matthe...   \n",
       "36640            Chandioux, John  and\\nGrimaila, Annette   \n",
       "36641  Atwell, Katherine  and\\nSicilia, Anthony  and\\...   \n",
       "\n",
       "                                                   title     pages  \\\n",
       "0      A Study on Word Similarity using Context Vecto...    37--58   \n",
       "1               Bootstrapping Large Sense Tagged Corpora      None   \n",
       "2      Headerless, Quoteless, but not Hopeless? Using...  327--335   \n",
       "3      Aligning Words in {E}nglish-{H}indi Parallel C...  115--118   \n",
       "4      Proposal of a very-large-corpus acquisition me...      None   \n",
       "...                                                  ...       ...   \n",
       "36637  Who, What, When, Where, Why? Comparing Multipl...  423--431   \n",
       "36638  Using Non-Local Features to Improve Named Enti...  303--310   \n",
       "36639  Scoring Interactional Aspects of Human-Machine...  103--109   \n",
       "36640                                           {LEXIUM}      None   \n",
       "36641  The Change that Matters in Discourse Parsing: ...  824--845   \n",
       "\n",
       "                                    doi number volume journal editor  isbn  \n",
       "0                                  None   None   None    None   None  None  \n",
       "1                                  None   None   None    None   None  None  \n",
       "2                                  None   None   None    None   None  None  \n",
       "3                                  None   None   None    None   None  None  \n",
       "4                                  None   None   None    None   None  None  \n",
       "...                                 ...    ...    ...     ...    ...   ...  \n",
       "36637                              None   None   None    None   None  None  \n",
       "36638  http://hdl.handle.net/2065/29132   None   None    None   None  None  \n",
       "36639              10.18653/v1/W19-5913   None   None    None   None  None  \n",
       "36640                              None   None   None    None   None  None  \n",
       "36641  10.18653/v1/2022.findings-acl.68   None   None    None   None  None  \n",
       "\n",
       "[36642 rows x 21 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = len(df)\n",
    "\n",
    "# cut df to half of the length\n",
    "df = df.head(length//2)\n",
    "\n",
    "# cut all abstact to 1000 characters\n",
    "df['abstract'] = df['abstract'].str.slice(0, 100)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        100\n",
      "1          0\n",
      "2        100\n",
      "3        100\n",
      "4          0\n",
      "        ... \n",
      "36637    100\n",
      "36638    100\n",
      "36639    100\n",
      "36640      0\n",
      "36641    100\n",
      "Name: abstract, Length: 36642, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print abstracts length\n",
    "print(df['abstract'].str.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "PineconeApiException",
     "evalue": "(413)\nReason: Request Entity Too Large\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'X-Cloud-Trace-Context': 'ebb6668a2f2ca9cdd1a978b9c6120d4b', 'Date': 'Sat, 14 Dec 2024 15:09:22 GMT', 'Server': 'Google Frontend', 'Content-Length': '56', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: Failed to buffer the request body: length limit exceeded\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPineconeApiException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultilingual-e5-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEND\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/inference.py:91\u001b[0m, in \u001b[0;36mInference.embed\u001b[0;34m(self, model, inputs, parameters)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     request_body \u001b[38;5;241m=\u001b[39m EmbedRequest(\n\u001b[1;32m     87\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     88\u001b[0m         inputs\u001b[38;5;241m=\u001b[39membeddings_inputs,\n\u001b[1;32m     89\u001b[0m     )\n\u001b[0;32m---> 91\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m EmbeddingsList(embeddings_list\u001b[38;5;241m=\u001b[39membeddings_list)\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/core/client/api_client.py:861\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This method is invoked when endpoints are called\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/core/client/api/inference_api.py:93\u001b[0m, in \u001b[0;36mInferenceApi.__init__.<locals>.__embed\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_check_return_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_check_return_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/core/client/api_client.py:934\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m     header_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mselect_header_content_type(\n\u001b[1;32m    930\u001b[0m         content_type_headers_list\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m header_list\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendpoint_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_check_return_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollection_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/core/client/api_client.py:435\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api,\n\u001b[1;32m    456\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m     ),\n\u001b[1;32m    474\u001b[0m )\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/core/client/api_client.py:218\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    217\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n\u001b[1;32m    222\u001b[0m return_data \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/core/client/api_client.py:206\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    202\u001b[0m     url \u001b[38;5;241m=\u001b[39m _host \u001b[38;5;241m+\u001b[39m resource_path\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    217\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/core/client/api_client.py:515\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mOPTIONS(\n\u001b[1;32m    506\u001b[0m         url,\n\u001b[1;32m    507\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    513\u001b[0m     )\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mPUT(\n\u001b[1;32m    526\u001b[0m         url,\n\u001b[1;32m    527\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    532\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/core/client/rest.py:398\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPOST\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m ):\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone_plugins/inference/core/client/rest.py:310\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m599\u001b[39m:\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mPineconeApiException\u001b[0m: (413)\nReason: Request Entity Too Large\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'X-Cloud-Trace-Context': 'ebb6668a2f2ca9cdd1a978b9c6120d4b', 'Date': 'Sat, 14 Dec 2024 15:09:22 GMT', 'Server': 'Google Frontend', 'Content-Length': '56', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: Failed to buffer the request body: length limit exceeded\n"
     ]
    }
   ],
   "source": [
    "embeddings = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[row['abstract'] for _, row in df.iterrows()],\n",
    "    parameters={\n",
    "        \"input_type\": \"passage\",\n",
    "        \"truncate\": \"END\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingsList(\n",
       "  model='multilingual-e5-large',\n",
       "  vector_type='dense',\n",
       "  data=[\n",
       "    {'vector_type': dense, 'values': [0.0005664825439453125, -0.025634765625, ..., -0.019683837890625, -0.0157012939453125]},\n",
       "    {'vector_type': dense, 'values': [0.048675537109375, -0.033416748046875, ..., -0.019744873046875, 0.024078369140625]},\n",
       "    ... (6 more embeddings) ...,\n",
       "    {'vector_type': dense, 'values': [0.02337646484375, -0.0054168701171875, ..., -0.041595458984375, 0.03546142578125]},\n",
       "    {'vector_type': dense, 'values': [0.033172607421875, -0.019378662109375, ..., -0.0194091796875, 0.0030612945556640625]}\n",
       "  ],\n",
       "  usage={'total_tokens': 3596}\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach metadata to the embeddings before uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for i, e in enumerate(embeddings):\n",
    "    vectors.append({\n",
    "        \"id\": df.iloc[i]['acl_id'],\n",
    "        \"values\": e.values,\n",
    "        \"metadata\": {\n",
    "            'title': df.iloc[i]['title'],\n",
    "            'author': df.iloc[i]['author'],\n",
    "            'url': df.iloc[i]['url'],\n",
    "            'abstract': df.iloc[i]['abstract']\n",
    "            }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload embeddings to the Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 10}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pinecone_index.upsert(vectors=vectors)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of querying the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[query],\n",
    "    parameters={\n",
    "        \"input_type\": \"query\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query similar records in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pinecone_index.query(\n",
    "    vector=x[0].values,\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'O02-2002',\n",
       "              'metadata': {'abstract': 'There is a need to measure word '\n",
       "                                       'similarity when processing natural '\n",
       "                                       'languages, especially when using '\n",
       "                                       'generalization, classification, or '\n",
       "                                       'example -based approaches. Usually, '\n",
       "                                       'measures of similarity between two '\n",
       "                                       'words are defined according to the '\n",
       "                                       'distance between their semantic '\n",
       "                                       'classes in a semantic taxonomy . The '\n",
       "                                       'taxonomy approaches are more or less '\n",
       "                                       'semantic -based that do not consider '\n",
       "                                       'syntactic similarit ies. However, in '\n",
       "                                       'real applications, both semantic and '\n",
       "                                       'syntactic similarities are required '\n",
       "                                       'and weighted differently. Word '\n",
       "                                       'similarity based on context vectors is '\n",
       "                                       'a mixture of syntactic and semantic '\n",
       "                                       'similarit ies. In this paper, we '\n",
       "                                       'propose using only syntactic related '\n",
       "                                       'co-occurrences as context vectors and '\n",
       "                                       'adopt information theoretic models to '\n",
       "                                       'solve the problems of data sparseness '\n",
       "                                       'and characteristic precision. The '\n",
       "                                       'probabilistic distribution of '\n",
       "                                       'co-occurrence context features is '\n",
       "                                       'derived by parsing the contextual '\n",
       "                                       'environment of each word , and all the '\n",
       "                                       'context features are adjusted '\n",
       "                                       'according to their IDF (inverse '\n",
       "                                       'document frequency) values. The '\n",
       "                                       'agglomerative clustering algorithm is '\n",
       "                                       'applied to group similar words '\n",
       "                                       'according to their similarity values. '\n",
       "                                       'It turns out that words with similar '\n",
       "                                       'syntactic categories and semantic '\n",
       "                                       'classes are grouped together.',\n",
       "                           'author': 'Chen, Keh-Jiann  and\\nYou, Jia-Ming',\n",
       "                           'title': 'A Study on Word Similarity using Context '\n",
       "                                    'Vector Models',\n",
       "                           'url': 'https://aclanthology.org/O02-2002'},\n",
       "              'score': 0.918417215,\n",
       "              'values': []},\n",
       "             {'id': 'W05-0819',\n",
       "              'metadata': {'abstract': 'In this paper, we describe a word '\n",
       "                                       'alignment algorithm for English-Hindi '\n",
       "                                       'parallel data. The system was '\n",
       "                                       'developed to participate in the shared '\n",
       "                                       'task on word alignment for languages '\n",
       "                                       'with scarce resources at the ACL 2005 '\n",
       "                                       'workshop, on \"Building and using '\n",
       "                                       'parallel texts: data driven machine '\n",
       "                                       'translation and beyond\". Our word '\n",
       "                                       'alignment algorithm is based on a '\n",
       "                                       'hybrid method which performs local '\n",
       "                                       'word grouping on Hindi sentences and '\n",
       "                                       'uses other methods such as dictionary '\n",
       "                                       'lookup, transliteration similarity, '\n",
       "                                       'expected English words and nearest '\n",
       "                                       'aligned neighbours. We trained our '\n",
       "                                       'system on the training data provided '\n",
       "                                       'to obtain a list of named entities and '\n",
       "                                       'cognates and to collect rules for '\n",
       "                                       'local word grouping in Hindi '\n",
       "                                       'sentences. The system scored 77.03% '\n",
       "                                       'precision and 60.68% recall on the '\n",
       "                                       'shared task unseen test data.',\n",
       "                           'author': 'Aswani, Niraj  and\\nGaizauskas, Robert',\n",
       "                           'title': 'Aligning Words in {E}nglish-{H}indi '\n",
       "                                    'Parallel Corpora',\n",
       "                           'url': 'https://aclanthology.org/W05-0819'},\n",
       "              'score': 0.842938542,\n",
       "              'values': []},\n",
       "             {'id': 'W05-0818',\n",
       "              'metadata': {'abstract': 'In this paper we describe LIHLA, a '\n",
       "                                       'lexical aligner which uses bilingual '\n",
       "                                       'probabilistic lexicons generated by a '\n",
       "                                       'freely available set of tools '\n",
       "                                       '(NATools) and languageindependent '\n",
       "                                       'heuristics to find links between '\n",
       "                                       'single words and multiword units in '\n",
       "                                       'sentence-aligned parallel texts. The '\n",
       "                                       'method has achieved an alignment error '\n",
       "                                       'rate of 22.72% and 44.49% on '\n",
       "                                       'English-Inuktitut and Romanian-English '\n",
       "                                       'parallel sentences, respectively.',\n",
       "                           'author': 'Caseli, Helena M.  and\\n'\n",
       "                                     'Nunes, Maria G. V.  and\\n'\n",
       "                                     'Forcada, Mikel L.',\n",
       "                           'title': '{LIHLA}: Shared Task System Description',\n",
       "                           'url': 'https://aclanthology.org/W05-0818'},\n",
       "              'score': 0.829676628,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete all records from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete all vectors\n",
    "# pinecone_index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ['OPEN_AI_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on\n",
    "a given context. \n",
    "\n",
    "Answer the question based on the context. If you can't answer the\n",
    "question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "example_context = \"Word similarity is important in natural language processing because it helps in generalization, classification, and example-based approaches.\"\n",
    "example_question = \"Why is word similarity important in natural language processing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_with_context = template.format(context=example_context, question=example_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[ \n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query_with_context,\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='Word similarity is important in natural language processing because it aids in generalization, classification, and example-based approaches.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain to me word similarity when processing natural languages, in Spanish.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform question to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[question],\n",
    "    parameters={\n",
    "        \"input_type\": \"query\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingsList(\n",
       "  model='multilingual-e5-large',\n",
       "  vector_type='dense',\n",
       "  data=[\n",
       "    {'vector_type': dense, 'values': [0.0121917724609375, -0.0251617431640625, ..., -0.0251617431640625, -0.0377197265625]}\n",
       "  ],\n",
       "  usage={'total_tokens': 22}\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the vector DB for similar records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pinecone_index.query(\n",
    "    vector=x[0].values,\n",
    "    top_k=1,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.matches[0].metadata['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up prompt for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_vectors_context = results.matches[0].metadata['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_with_context = template.format(context=similar_vectors_context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query_with_context,\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='La similitud de palabras al procesar idiomas naturales se mide generalmente según la distancia entre sus clases semánticas en una taxonomía semántica. Las medidas de similitud suelen ser semánticas y no consideran similitudes sintácticas. Sin embargo, en aplicaciones reales, se requieren similitudes semánticas y sintácticas, que se ponderan de manera diferente. La similitud de palabras basada en vectores de contexto combina sintácticas y semánticas. En este enfoque, se proponen co-ocurrencias relacionadas sintácticamente como vectores de contexto y se utilizan modelos teóricos de la información para abordar problemas de escasez de datos y precisión. Se ajustan las características de contexto según sus valores de IDF (frecuencia inversa de documentos), y se aplica un algoritmo de clustering aglomerativo para agrupar palabras similares. Así, las palabras con categorías sintácticas y clases semánticas similares se agrupan juntas.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_investigacion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
