{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Sistema de asistencia en investigación académica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasmautone/Github-Projects/NLP_Datasets/rag-investigacion-academica/rag_investigacion_env/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "import json\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus - ACL Anthology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../acl-publication-info.74k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "      <th>corpus_paper_id</th>\n",
       "      <th>pdf_hash</th>\n",
       "      <th>numcitedby</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>address</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>booktitle</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>pages</th>\n",
       "      <th>doi</th>\n",
       "      <th>number</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>editor</th>\n",
       "      <th>isbn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O02-2002</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>18022704</td>\n",
       "      <td>0b09178ac8d17a92f16140365363d8df88c757d0</td>\n",
       "      <td>14</td>\n",
       "      <td>https://aclanthology.org/O02-2002</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>International Journal of Computational Linguis...</td>\n",
       "      <td>Chen, Keh-Jiann  and\\nYou, Jia-Ming</td>\n",
       "      <td>A Study on Word Similarity using Context Vecto...</td>\n",
       "      <td>37--58</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L02-1310</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8220988</td>\n",
       "      <td>8d5e31610bc82c2abc86bc20ceba684c97e66024</td>\n",
       "      <td>93</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the Third International Confere...</td>\n",
       "      <td>Mihalcea, Rada F.</td>\n",
       "      <td>Bootstrapping Large Sense Tagged Corpora</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R13-1042</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>16703040</td>\n",
       "      <td>3eb736b17a5acb583b9a9bd99837427753632cdb</td>\n",
       "      <td>10</td>\n",
       "      <td>https://aclanthology.org/R13-1042</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the International Conference Re...</td>\n",
       "      <td>Jamison, Emily  and\\nGurevych, Iryna</td>\n",
       "      <td>Headerless, Quoteless, but not Hopeless? Using...</td>\n",
       "      <td>327--335</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W05-0819</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>1215281</td>\n",
       "      <td>b20450f67116e59d1348fc472cfc09f96e348f55</td>\n",
       "      <td>15</td>\n",
       "      <td>https://aclanthology.org/W05-0819</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the {ACL} Workshop on Building ...</td>\n",
       "      <td>Aswani, Niraj  and\\nGaizauskas, Robert</td>\n",
       "      <td>Aligning Words in {E}nglish-{H}indi Parallel C...</td>\n",
       "      <td>115--118</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L02-1309</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>18078432</td>\n",
       "      <td>011e943b64a78dadc3440674419821ee080f0de3</td>\n",
       "      <td>12</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the Third International Confere...</td>\n",
       "      <td>Suyaga, Fumiaki  and\\nTakezawa, Toshiyuki  and...</td>\n",
       "      <td>Proposal of a very-large-corpus acquisition me...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R13-1044</td>\n",
       "      <td>The paper 1 presents a rule-based approach to ...</td>\n",
       "      <td>The paper 1 presents a rule-based approach to ...</td>\n",
       "      <td>2491460</td>\n",
       "      <td>c0f1047fe0f95c367184d494e78bb07b11ee3608</td>\n",
       "      <td>2</td>\n",
       "      <td>https://aclanthology.org/R13-1044</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the International Conference Re...</td>\n",
       "      <td>K{\\k{e}}dzia, Pawe{\\l}  and\\nMaziarz, Marek</td>\n",
       "      <td>Recognizing semantic relations within {P}olish...</td>\n",
       "      <td>342--349</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>W05-0818</td>\n",
       "      <td>In this paper we describe LIHLA, a lexical ali...</td>\n",
       "      <td>In this paper we describe LIHLA, a lexical ali...</td>\n",
       "      <td>15322146</td>\n",
       "      <td>ff3f05120d24e5dac2879f25402993bc6355f780</td>\n",
       "      <td>5</td>\n",
       "      <td>https://aclanthology.org/W05-0818</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the {ACL} Workshop on Building ...</td>\n",
       "      <td>Caseli, Helena M.  and\\nNunes, Maria G. V.  an...</td>\n",
       "      <td>{LIHLA}: Shared Task System Description</td>\n",
       "      <td>111--114</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L02-1313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>649937</td>\n",
       "      <td>c5c1643517ee6646c47b4ee2b8443d4f62ee1ae5</td>\n",
       "      <td>4</td>\n",
       "      <td>http://www.lrec-conf.org/proceedings/lrec2002/...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>Las Palmas, Canary Islands - Spain</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the Third International Confere...</td>\n",
       "      <td>Baldwin, Timothy  and\\nBilac, Slaven  and\\nOku...</td>\n",
       "      <td>Enhanced {J}apanese Electronic Dictionary Look-up</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>R13-1045</td>\n",
       "      <td>We describe an approach to building a morpholo...</td>\n",
       "      <td>We describe an approach to building a morpholo...</td>\n",
       "      <td>690455</td>\n",
       "      <td>0b125557ba23075532380e88fb990933838975b7</td>\n",
       "      <td>2</td>\n",
       "      <td>https://aclanthology.org/R13-1045</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the International Conference Re...</td>\n",
       "      <td>Khaliq, Bilal  and\\nCarroll, John</td>\n",
       "      <td>Unsupervised Induction of {A}rabic Root and Pa...</td>\n",
       "      <td>350--356</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>W05-0821</td>\n",
       "      <td>Statistical machine translation systems use a ...</td>\n",
       "      <td>Statistical machine translation systems use a ...</td>\n",
       "      <td>1966857</td>\n",
       "      <td>2a05c9c5373a3e1e01b8161e6687b960ab3d2ff5</td>\n",
       "      <td>55</td>\n",
       "      <td>https://aclanthology.org/W05-0821</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>Proceedings of the {ACL} Workshop on Building ...</td>\n",
       "      <td>Kirchhoff, Katrin  and\\nYang, Mei</td>\n",
       "      <td>Improved Language Modeling for Statistical Mac...</td>\n",
       "      <td>125--128</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acl_id                                           abstract  \\\n",
       "0  O02-2002  There is a need to measure word similarity whe...   \n",
       "1  L02-1310                                                      \n",
       "2  R13-1042  Thread disentanglement is the task of separati...   \n",
       "3  W05-0819  In this paper, we describe a word alignment al...   \n",
       "4  L02-1309                                                      \n",
       "5  R13-1044  The paper 1 presents a rule-based approach to ...   \n",
       "6  W05-0818  In this paper we describe LIHLA, a lexical ali...   \n",
       "7  L02-1313                                                      \n",
       "8  R13-1045  We describe an approach to building a morpholo...   \n",
       "9  W05-0821  Statistical machine translation systems use a ...   \n",
       "\n",
       "                                           full_text  corpus_paper_id  \\\n",
       "0  There is a need to measure word similarity whe...         18022704   \n",
       "1                                                             8220988   \n",
       "2  Thread disentanglement is the task of separati...         16703040   \n",
       "3  In this paper, we describe a word alignment al...          1215281   \n",
       "4                                                            18078432   \n",
       "5  The paper 1 presents a rule-based approach to ...          2491460   \n",
       "6  In this paper we describe LIHLA, a lexical ali...         15322146   \n",
       "7                                                              649937   \n",
       "8  We describe an approach to building a morpholo...           690455   \n",
       "9  Statistical machine translation systems use a ...          1966857   \n",
       "\n",
       "                                   pdf_hash  numcitedby  \\\n",
       "0  0b09178ac8d17a92f16140365363d8df88c757d0          14   \n",
       "1  8d5e31610bc82c2abc86bc20ceba684c97e66024          93   \n",
       "2  3eb736b17a5acb583b9a9bd99837427753632cdb          10   \n",
       "3  b20450f67116e59d1348fc472cfc09f96e348f55          15   \n",
       "4  011e943b64a78dadc3440674419821ee080f0de3          12   \n",
       "5  c0f1047fe0f95c367184d494e78bb07b11ee3608           2   \n",
       "6  ff3f05120d24e5dac2879f25402993bc6355f780           5   \n",
       "7  c5c1643517ee6646c47b4ee2b8443d4f62ee1ae5           4   \n",
       "8  0b125557ba23075532380e88fb990933838975b7           2   \n",
       "9  2a05c9c5373a3e1e01b8161e6687b960ab3d2ff5          55   \n",
       "\n",
       "                                                 url  \\\n",
       "0                  https://aclanthology.org/O02-2002   \n",
       "1  http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "2                  https://aclanthology.org/R13-1042   \n",
       "3                  https://aclanthology.org/W05-0819   \n",
       "4  http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "5                  https://aclanthology.org/R13-1044   \n",
       "6                  https://aclanthology.org/W05-0818   \n",
       "7  http://www.lrec-conf.org/proceedings/lrec2002/...   \n",
       "8                  https://aclanthology.org/R13-1045   \n",
       "9                  https://aclanthology.org/W05-0821   \n",
       "\n",
       "                                        publisher  \\\n",
       "0                                            None   \n",
       "1  European Language Resources Association (ELRA)   \n",
       "2                   INCOMA Ltd. Shoumen, BULGARIA   \n",
       "3       Association for Computational Linguistics   \n",
       "4  European Language Resources Association (ELRA)   \n",
       "5                   INCOMA Ltd. Shoumen, BULGARIA   \n",
       "6       Association for Computational Linguistics   \n",
       "7  European Language Resources Association (ELRA)   \n",
       "8                   INCOMA Ltd. Shoumen, BULGARIA   \n",
       "9       Association for Computational Linguistics   \n",
       "\n",
       "                              address  year  ...  \\\n",
       "0                                None  2002  ...   \n",
       "1  Las Palmas, Canary Islands - Spain  2002  ...   \n",
       "2                    Hissar, Bulgaria  2013  ...   \n",
       "3                 Ann Arbor, Michigan  2005  ...   \n",
       "4  Las Palmas, Canary Islands - Spain  2002  ...   \n",
       "5                    Hissar, Bulgaria  2013  ...   \n",
       "6                 Ann Arbor, Michigan  2005  ...   \n",
       "7  Las Palmas, Canary Islands - Spain  2002  ...   \n",
       "8                    Hissar, Bulgaria  2013  ...   \n",
       "9                 Ann Arbor, Michigan  2005  ...   \n",
       "\n",
       "                                           booktitle  \\\n",
       "0  International Journal of Computational Linguis...   \n",
       "1  Proceedings of the Third International Confere...   \n",
       "2  Proceedings of the International Conference Re...   \n",
       "3  Proceedings of the {ACL} Workshop on Building ...   \n",
       "4  Proceedings of the Third International Confere...   \n",
       "5  Proceedings of the International Conference Re...   \n",
       "6  Proceedings of the {ACL} Workshop on Building ...   \n",
       "7  Proceedings of the Third International Confere...   \n",
       "8  Proceedings of the International Conference Re...   \n",
       "9  Proceedings of the {ACL} Workshop on Building ...   \n",
       "\n",
       "                                              author  \\\n",
       "0                Chen, Keh-Jiann  and\\nYou, Jia-Ming   \n",
       "1                                  Mihalcea, Rada F.   \n",
       "2               Jamison, Emily  and\\nGurevych, Iryna   \n",
       "3             Aswani, Niraj  and\\nGaizauskas, Robert   \n",
       "4  Suyaga, Fumiaki  and\\nTakezawa, Toshiyuki  and...   \n",
       "5        K{\\k{e}}dzia, Pawe{\\l}  and\\nMaziarz, Marek   \n",
       "6  Caseli, Helena M.  and\\nNunes, Maria G. V.  an...   \n",
       "7  Baldwin, Timothy  and\\nBilac, Slaven  and\\nOku...   \n",
       "8                  Khaliq, Bilal  and\\nCarroll, John   \n",
       "9                  Kirchhoff, Katrin  and\\nYang, Mei   \n",
       "\n",
       "                                               title     pages   doi number  \\\n",
       "0  A Study on Word Similarity using Context Vecto...    37--58  None   None   \n",
       "1           Bootstrapping Large Sense Tagged Corpora      None  None   None   \n",
       "2  Headerless, Quoteless, but not Hopeless? Using...  327--335  None   None   \n",
       "3  Aligning Words in {E}nglish-{H}indi Parallel C...  115--118  None   None   \n",
       "4  Proposal of a very-large-corpus acquisition me...      None  None   None   \n",
       "5  Recognizing semantic relations within {P}olish...  342--349  None   None   \n",
       "6            {LIHLA}: Shared Task System Description  111--114  None   None   \n",
       "7  Enhanced {J}apanese Electronic Dictionary Look-up      None  None   None   \n",
       "8  Unsupervised Induction of {A}rabic Root and Pa...  350--356  None   None   \n",
       "9  Improved Language Modeling for Statistical Mac...  125--128  None   None   \n",
       "\n",
       "  volume journal editor  isbn  \n",
       "0   None    None   None  None  \n",
       "1   None    None   None  None  \n",
       "2   None    None   None  None  \n",
       "3   None    None   None  None  \n",
       "4   None    None   None  None  \n",
       "5   None    None   None  None  \n",
       "6   None    None   None  None  \n",
       "7   None    None   None  None  \n",
       "8   None    None   None  None  \n",
       "9   None    None   None  None  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first 10 rows\n",
    "df_reduced = df.head(10)\n",
    "df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Study on Word Similarity using Context Vector Models\n",
      "There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.\n",
      "There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together. Introduction It is well known that word-sense is defined by a word's co-occurrence context. The context vectors of a word are defined as the probabilistic distributions of its left and right co-occurrence contexts. Conventionally , the similarity between two context vectors is measured based on their cosine distance [Alshawi and Cater, 1994; Grishman and Sterling, 1994; Pereira et al., 1993; Ruge, 1992; Salton, 1989] . However, the conventional measurement * Institute of Information Science, Academia Sinica E-mail: kchen@iis.sinica.edu.tw; swimming@hp.iis.sinica.edu.tw suffers from the following drawbacks. First of all, the information in the context vectors is vague. All co-occurrence words are collected without distinguishing whether they are syntactically or semantically related. Second, the coordinates are not pair-wise independent (i.e., the axes are not orthogonal), and it is hard to apply singular value decomposition to find the orthogonal vectors [Schutze, 1992] . In this paper, we propose to use only syntactic related co-occurrences as context vectors [Dekang Lin, 1998 ] and adopt information theoretic models to solve the above problems. In our study, the context vectors of a word are defined as the probabilistic distributions of its thematic roles and left/right co-occurrence semantic classes. The context features are derived from a treebank. All context features are weighted according to their TF × IDF values (the product of the term frequency and inverse document frequency) [Salton, 1989] . For the context features, the Cilin semantic classes (a Chinese thesaurus) are adopted. The Cilin semantic classes are divided into 4 different levels of granularity. In order to cope with the data sparseness problem, the weighted average of the similarity values at four different levels will be the similarity measure of two words. The weight for each level is equal to the information-content of that level [Shannon, 1948; Manning and Schutze 1999] . A agglomerative clustering algorithm is applied to group similar words according to the above defined similarity measure. Obviously, words with similar behavior in the corpus will be grouped together. We have compared the clustering results to the Cilin classifications. It turns out that words in the same synonym class and with the same syntactic categories have higher similarity values than the words with different syntactic categories. Data Resources Ideally, to derive context vectors, a large corpus with semantic tags is required. Furthermore, to extract co-occurrence words along with their exact syntactic and semantic relations, the corpus structure has to be annotated. However, such an ideal corpus does not exist. Therefore, in this paper we will adopt the resources that are available and try to derive a useful but imperfect Chinese tree bank. Since the similarity measure based on the vector space model is a rough estimation, minor errors made at the stage of context vector extraction are acceptable. Sinica Corpus The Sinica corpus contains 12,532 documents and nearly 5 million words. Each sentence in the corpus was parsed by a rule parser [Chen, 1996] . The parsed trees were tagged with the structure brackets, syntactic categories and thematic roles of each constituent [Huang et al., 2000] as exemplified below, (Sinica corpus: http://www.sinica.edu.tw/ftms -bin/kiwi.sh): Original sentence: 小 'small' 狗 'dog' 跳舞 'dance' Parsed tree : S(Agent:NP:(Property:Adj:'小 small'| Head: N: ' 狗 dog' )|Head:V: ' 跳舞 dance') Although these labels may not be exactly correct, we believe that, even with these minor errors, the majority of word -to-word relations extracted from the trees are correct. However, the semantic label is not provided for each word in the parsed trees. In this paper, we will use Cilin classifications for semantic labeling. Cilin-a Chinese Thesaurus Cilin provides the Mandarin synonym sets in a hierarchical structure [Mei et. al., 1984] . It contains 51,708 Chinese words, and 3918 classes. There are five levels in the Cilin semantic hierarchy, denoted in the format L 1 -L 2 -L 3 -L 4 -L 5 . For example, the Cilin class of the word 我們 'we' is \"A -a-02-2-01\". In level 1, \"A\", denotes the semantic class of human; in level 2, \"a\", indicates a group of general terms; level 3, \"02\", means pronouns in the first person, and in level 4, \"2\" represents the plural property. In level 5, \"01\" represents the order rank in the level 4 group. This means that \"01\" in level 5 is the first prototypical concept representation of \"A-a-02-2\". In the rest of this paper, only the first four levels will be used. The fifth level is for sense disambiguation only (section 2.3). Sense Disambiguation A polysemous word has more than one Cilin semantic class. In order to tag appropriate Cilin classes, we have designed a simple sense tagging method as follows [Wilks, 1999] . The sense tagging algorithm is based on the facts that the syntactic categories of each word in the tree bank are assigned uniquely, and that each Cilin class has its own major syntactic category. If a word has multiple Cilin classes, we select the sense class whose major syntactic category is the same as the tagged category of this word. For example , 計畫 \" Jie -Hwa\" has two meanings. One is for \"project\" as a noun and the other is \"attempt\", therefore, if 計畫 \"Jie -Hwa\" was tagged with a noun category, we will assign the Cilin class whose major category is \"project\". Sense ambiguity can be distinguished by measure of syntactic properties for most words. However, there are still cases in which the syntactic category constraints cannot resolve the sense ambiguities. Then, we simply choose the prototypical sense class, i.e, the word that has the highest rank in this sense class with respect to all its sense classes in Cilin. The Extraction of Co-occurrence Data The extracted syntactically related pairs have either a head-modifier relation or head-argument relation. For instance, two syntactically related pairs extracted from the example in section 2.1 are: (<Thematic role > <Cilin> <word1>), (<Thematic role> <Cilin> <word2>) (agent B -i-07-2 狗'dog'), (Head(S) H-h-04-2 跳舞'dance' ) (property E -a-03-3 小'small'), (Head(NP) B -i-07-2 狗'dog') The context data of the word 1 狗 \"dog\" consists of its thematic role \"agent\" and the Cilin class \"B-i-07-2\" ; the word 2 跳舞 'dance' consists the thematic role \"Head(S)\" and the Cilin class \"H-h-04-2\" and so on. The word 小 \"small\" and 跳舞 \"dance\" are not syntactically related even though they co-occur. Therefore , they will not be extracted. Context Vector Model There are three context vectors of a word: role vector, left context vector and right context vector. The role vector is a fixed 48-dimension vector, and each dimension value is equal to the probabilistic distribution of its thematic roles. The left/right context vectors are closer to the probabilistic distributions of its left/right co-occurrence words and their semantic classes. The role vector characterizes a word based more on syntax and less on semantics, but the left/right context vectors are just the opposite. The cosine distance between their context vectors is a measure of the similarity of the two words. We will illustrate the derivations of context vectors and their similarity rating with a simplified example using (貓 \"cat\", 狗 \"dog\"). The role vector of \"dog\" is {127, 207, 169… , 0} 48 , which represents the values of \"agent\", \"goal\", \"theme\"… and \"topic\" respectively, generated by Equation (1). The role vector of \"cat\" is {28, 73, 56… , 0} 48 , which is also acquired by Equation (1). Role vector of word W = { V 1 , V 2 ,… ,V 48 } 48 Vi = Frequency (R i ) × log (1/P i ) (1) R i : We label thematic roles \"agent\", \"goal\"… and \"topic\" from R 1 to R 48 listed in Table2 in the Appendix. Frequency (R i ): The frequency of R i played by word W in the corpus. P i : = Total frequency of R i in the corpus / Total frequency of all roles in the corpus. log(1/P i ): The information-context of R i [Shannon, 1948; Manning and Schutze, 1999] The derivation of left/right context vectors is a bit more complicated. The syntactically related co-occurrence word pairs are derived first as illustrated in section 2.4. We will illustrate the derivation of the left context vector only. The right context vector can be derived simila rly. The left co-occurrence word vector of word W is generated from frequency(word i ), where word i precedes and is syntactically related to W in the corpus. Due to the data sparseness problem, the feature dimensions of context vectors are generalized into C ilin classes instead of co-occurrence words. The generalization process reduces the effect of data sparseness. On the other hand, it also reduces the precision of characterization since each word has different information content and two words that have the same co-occurrence semantic classes may not share the same co-occurrence words. In order to resolve the above dilemma, when we compare the similarity between word X and word Y , 4 levels of left context vectors and right context vectors for word X and word Y are created 1 . The weighting of each feature dimension is adjusted using the TF*IDF value if word X and word Y have shared context words. Equation ( 2 ) illustrates the creation of the 4 th level left context vector of word X . The other context vectors for word X and word Y are created by a similar way. Left context vector of word X = {f1, f2,… ,f 3918 } L4 Where fi = Sum of { TF(word j ) × IDF(word j ) if word X has the same neighbor word j with word Y TF(word j ) if word X does not have the same neighbor word j with word Y } ; for every left co-occurrence word j with the ith Cilin semantic class. (2) TF(word j ): the frequency of pair ( word j , Cilin(word j ) ) in wordx' s co-occurrence context. IDF(word j ): -log(the number of the documents that contains the word j /total document number of the corpus) In Equation ( 2 ), we adjust the term weight using TF × IDF, which is commonly used in the field of information retrieval [Salton, 1989] to adjust the discrimination power of each feature dimension. We will examine the difference in the adjustment of weights using TF × IDF and TF in section 4.2. We will next give a simplified example. Assume that the word 狗 \"dog\" has only three left syntactically related words: ( 小 \"small\" Ea033 ) with frequency 30, ( 可愛 \"cute\" Ed401 ) with frequency 5 and ( 養 \"raise\" Ib011 ) with frequency 10; and assume that the word 貓 \"cat\" has only two left syntactically related words: ( 黑 \"black\" Ec043 ) and ( 養 \"raise\" Ib011 ). Assume that we are measuring the similarity between 狗 \"dog\" and 貓 \"cat\". Then, we can compute the left context data of 狗 \"dog\" as {TF(Aa011),… ,TF(Ea033),… ,TF(Ed041),… ,TF(Ib011) × IDF(養'raise') 2 , … , TF(La064) } L4 3 1 IDF(養'raise') = 4.188, the IDF values of all words range from 0.19 to 9.12. 2 The granularities of the 4 levels of semantic classes are partially shown in Figure2. The four left context vectors and their dimensions are shown below and the right context vectors are similarly derived. <LeftCilin1> L1 A vector of 12 dimensions from \"A\" to \"L\". <LeftCilin2> L2 A vector of 94 dimensions from \"Aa\" to \"La\". <LeftCilin3> L3 A vector of 1428 dimensions from \"Aa01\" to \"La06\". <LeftCilin4> L4 A vector of 3918 dimensions from \"Aa011\" to \"La064\". Similarities between Two Context Vectors Once we know the feature vectors of these two words, we can calculate the cosine distance of two vectors as shown in Equation (3). vector A= <a1,a2… ,an>,vector B= <b1,b2… ,bn> ... ) , cos( 1 2 1 2 1 ∑ ∑ ∑ = = = × × = n i n i n i bi ai bi ai B A (3) Therefore, the similarity of the two words x and y can be calculated as the linear combination of the cosine distances of all the feature vectors as shown in the Equation 4 . The weight of each feature vector can be adjusted according to different requirements. For instance, if the syntactic similarity is more important, we can increase the weight w. On the other hand, if the semantic similarity is more important, the weights w1 to w4 can be increased. If more training data is available, the level 4 vector will be more reliable. Hence, the weight w4 should increase. (4) y)} > 4 RightCilin < x, > 4 RightCilin cos(< w42 + y) > LeftCilin4 < x, > LeftCilin4 (< cos w41 { w4 y)} > 3 RightCilin < x, > 3 RightCilin (< cos w32 + y) > LeftCilin3 < x, > LeftCilin3 (< cos w31 { w3 y)} > 2 RightCilin < x, > 2 RightCilin (< cos w22 + y) > LeftCilin2 < x, > LeftCilin2 (< cos w21 { 2 y)} > 1 RightCilin < x, > 1 RightCilin (< cos w12 + y) > LeftCilin1 < , x > LeftCilin1 (< cos w11 { 1 y) > vector role < x, > vector role (< cos w = y) (x, similarity × + × + × + × + × w w In the experiments, w = 0.3, w1 = 0.1 × 0.7, w2 = 0.1 × 0.7, w3 = 0.4 × 0.7, and w4 = 0.4 × 0.7. Similarity Clustering Because of the lack of objective standards for evaluating of similarity measures, a agglomerative clustering algorithm is applied to group similar words according to a similarity value. It turns out that words with similar syntactic usage and similar semantic classes are grouped together. We will evaluate our algorithm by comparing the automatic clustering results with manual classifications of Cilin. Clustering Algorithm To evaluate the proposed similarity measure, we tried to group words according to various parameters. We adopted bottom-up agglomerative clustering algorithm to group words. In order to compare the clustered results with Cilin classifications and reduce the data sparseness, we picked the 1000 highest frequency words in Cilin for testing. First of all, we produced a 1000 × 1000 symmetric similarity matrix called SMatrix, where SMatrix (x, y) = similarity 4 3 2 1 4 , 3 , 2 , 1 K RightCilin K n RighttCili K LeftCilin x K LeftCilin K RightCilin x K RightCilin K RightCilin K n RighttCili K LeftCilin x K LeftCilin K LeftCilin x K LeftCilin 2 1 > < = + + + + = > < + > < + > < + > < > < + > < = > < + > < + > < + > < > < + > < = [i] SMatrix[x] , q , p 1 q p 1 q ≤ ≤ ≤ ≤ < < × = ∑∑ = = p n m m p n n q 1 word contains ) wordj ( Group m 1 word )contains wordx ( Group x j 0 ) word , (word similarity [x] SMatrix[j] , q , p 1 q p 1 q ≤ ≤ ≤ ≤ < < × = ∑∑ = = p n m m p n Clustering Results vs Cilin Classification We will make a comparison between the clustering results and Cilin classifications. There are two simple examples shown in Figure 1 Among our 1000 testing words, the number of words that were clustered in the 4 th level of Cilin was 658; i.e., they were labeled with 459 different level-4 Cilin classes and among them, 342 classes contained only one testing word, and the classes with multiple testing words contained a total of 658 testing words. With the threshold=0.7, our method clustered 830 words, and only 167 words of them were clustered in the correct Cilin class. Therefore, by Equation ( 5 ), recall 4 = 167/658 = 0.25 and with Equation ( 6 ) precision 4 = 167/830 = 0.20. We adopted two methods for measuring similarity; one used Equation TF × IDF, and the other used Equation TF. The results are shown in Figure 3 to Figure 6 in the Appendix. We measured the performance by computing the F -score, which is (recall+precision)/2. We discovered that the best F-score of level1 was that 0.7648 located at a threshold equal to 0.65, the best F-score of level2 was that 0.5178 located at a threshold equal to 0.7, the best F-score of level3 was that 0.3165 located at a threshold equal to 0.8, and that the best F-score of level4 was that 0.2476 located at a the threshold equal to 0.8. All were obtained using TF × IDF strategy. Hence, we can see that the TF × IDF equation achieves better performance than the TF equation does. We list the detailed F-socre data for various parameters in appendix. Although the clustering results didn't fit Cilin completely, they are still alike to some degree. From the results, we find that they are similar to syntax taxonomy under a lower threshold and close to semantic taxonomy under higher thresholds. Cilin classifications re -examined To examine the practicability of our proposed method, we also inspected the similarity values of these 658 testing words which were clustered into 117 4 th level Cilin classes. For each semantic class, the average similarity between words in the class and their standard deviation was comp uted. The results are listed in Table1 in the Appendix. We expected that synonyms would have high similarity values, but this was not always the case. According to the assumption noted above, synonyms might have similar syntactic and semantic contexts in language use. Therefore, the average similarity should be pretty high, and the standard deviation should be quite low. However, some of the results didn't follow the assumption. We analyzed the data offer explanations in the following. The context s of the noun (思想, \"thinking\") and the verbs (考慮,考量,思考, \"think\", \"consider\", \"deliberate\") were quite different. As a result, the average similarity value was quite low, and the standard deviation was very high. A fter we remo ved the noun from the word-set, we recomputed the values and obtained the The results conform to our assumption. They also reveal that the context of synonyms may vary from POS to POS. b) Error in Cilin Classification: The classifications in Cilin could be arbitrary. For example, the three words, 數量 \"quantity\", 多少 \"how many\" and 人數 \"the number of people\", were classified in a Cilin group. They might be slightly related, but grouping them together seems inappropriate according to the following table : Word set Average similarity Stand deviation 數量,多少 人數 0.379825 0.253895 c) Different uses: Differences in their usage cause synonyms to behave differently . For example, when we measured the similarity of 美國 \"America\" to 日本 \"Japan\" and to 中國 \"China\", the results we obtained were 0.86 and 0.62,respectively, for each pair. Accroding to human intuition, they simply refer to names of countries and should not have such different similarity values. The reason for these result is that the corpus we adopted is an original Taiwan corpus. As a result, the usage of 中國 \"China\" is different from that of 美國 \"America\" and 日本 \"Japan\". d) Polysemy: The word senses that Cilin adopted were not those frequently used in the corpus. See the following table : . Word set Average similarity Stand deviation 十分,非常, 特別 0.45054 0.305209 Although the three words, 十分 \"very/ten points\", 非常 \"very\" and 特 別\"special, extraordinary\" might seem to be very close in meaning to \"very\", the polysemous word 特別 \"special, extraordinary\" is different in its major sense. This influenced the result. e) Words with similar contexts might not be synonyms: A disadvantage does exist when the context vector model is used. Words that are similar in terms of their context s might not be similar in meaning. For example, the similarity value of 結婚 \"marry\" and 長大 \"grow\" is 0.8139. Although the two words have similar contexts, they are not alike in meaning. Therefore, the vector space model should incorporate the taxonomy approach to solve this phenomenon. Conclusions In this paper, we have adopted the context vector model to measure word similarity. The following new features have been proposed to enhance the context vector models : a) The weights of features are adjusted by applying TF × IDF. b) Feature vectors are smoothed by using Cilin categories to reduce data sparseness. c) Syntactic and semantic similarity is balanced by using related syntactic contexts only. The performance of our method might have been influenced by the small scale of the Chinese corpus and accuracy of the extracted relations. Further more, Cilin was published a long time ago and has not been update recently, which may have influenced our results. However, our experimental results are encouraging. They supports the theory that using context vectors to measure similarity is feasible and worthy of further research. Appendix\n"
     ]
    }
   ],
   "source": [
    "# print title, abstract and full text of the first row\n",
    "print(df.iloc[0]['title'])\n",
    "print(df.iloc[0]['abstract'])\n",
    "print(df.iloc[0]['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a serverless index\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "index_name = \"academic-papers\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name, \n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    )\n",
    "\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)\n",
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to transform row to pinecone format\n",
    "def row_to_pinecone(row):\n",
    "    return {\n",
    "        \"id\": row['acl_id'],\n",
    "        \"values\": json.loads(row['embedding']),\n",
    "        \"metadata\": {\n",
    "            \"abstract\": row['abstract'],\n",
    "            \"title\": row['title'],\n",
    "            \"acl_id\": row['acl_id'],\n",
    "            \"corpus_paper_id\": row['corpus_paper_id'],\n",
    "            \"bd\": \"acl\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 63523}},\n",
      " 'total_vector_count': 63523}\n"
     ]
    }
   ],
   "source": [
    "print(pinecone_index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of querying the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I am doing a research about active learning in NLP. Can you help me find some papers about it?\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[query],\n",
    "    parameters={\n",
    "        \"input_type\": \"query\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query similar records in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pinecone_index.query(\n",
    "    vector=x[0].values,\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As supervised machine learning methods for addressing tasks in natural language processing (NLP) prove increasingly viable, the focus of attention is naturally shifted towards the creation of training data. The manual annotation of corpora is a tedious and time consuming process. To obtain high-quality annotated data constitutes a bottleneck in machine learning for NLP today. Active learning is one way of easing the burden of annotation. This paper presents a first probe into the NLP research community concerning the nature of the annotation projects undertaken in general, and the use of active learning as annotation support in particular.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.matches[0].metadata['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ['OPEN_AI_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"\"\"\n",
    "You are an assistant that provides help to a researcher that is looking for academic papers about a specific topic explained in a query.\n",
    "\n",
    "You have to base your answers on a context that contains the title and abstract of the 3 most relevant papers found to this query. \n",
    "\n",
    "Your tasks are:\n",
    "\n",
    "-Analyze the abstracts and find out if they are really relevant to the research topic included in the query.\n",
    "-Obtain the similarities and differences between the papers and the topic. Be as concise as possible and go straight to the point.\n",
    "\n",
    "If you think that any of the papers are not relevant to the query, reply \"I didn't find any relevant information to this topic\".\n",
    "\n",
    "If any of the papers are relevant to the query, reply with the following template:\n",
    "\n",
    "'I have found relevant information about your research topic in the following papers:\n",
    "    (For each relevant paper)\n",
    "    - Title: [title]\n",
    "    - Similarities: [Briefly list key similarities with the topic]\n",
    "    - Differences: [Briefly list key differences or gaps]\n",
    "'\n",
    "\n",
    "Context:\n",
    "\n",
    "    Title 1: {title1}\n",
    "    Abstract 1: {abstract1}\n",
    "\n",
    "    Title 2: {title2}\n",
    "    Abstract 2: {abstract2}\n",
    "\n",
    "    Title 3: {title3}\n",
    "    Abstract 3: {abstract3}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_with_context = template2.format(title1=results.matches[0].metadata['title'], \n",
    "                                      abstract1=results.matches[0].metadata['abstract'],\n",
    "                                      title2=results.matches[1].metadata['title'],  \n",
    "                                      abstract2=results.matches[1].metadata['abstract'],\n",
    "                                      title3=results.matches[2].metadata['title'], \n",
    "                                      abstract3=results.matches[2].metadata['abstract'], \n",
    "                                      query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are an assistant that provides help to a researcher that is looking for academic papers about a specific topic explained in a query.\\n\\nYou have to base your answers on a context that contains the title and abstract of the 3 most relevant papers found to this query. \\n\\nYour tasks are:\\n\\n-Analyze the abstracts and find out if they are really relevant to the research topic included in the query.\\n-Obtain the similarities and differences between the papers and the topic. Be as concise as possible and go straight to the point.\\n\\nIf you think that any of the papers are not relevant to the query, reply \"I didn\\'t find any relevant information to this topic\".\\n\\nIf any of the papers are relevant to the query, reply with the following template:\\n\\n\\'I have found relevant information about your research topic in the following papers:\\n    (For each relevant paper)\\n    - Title: [title]\\n    - Similarities: [Briefly list key similarities with the topic]\\n    - Differences: [Briefly list key differences or gaps]\\n\\'\\n\\nContext:\\n\\n    Title 1: A Web Survey on the Use of Active Learning to Support Annotation of Text Data\\n    Abstract 1: As supervised machine learning methods for addressing tasks in natural language processing (NLP) prove increasingly viable, the focus of attention is naturally shifted towards the creation of training data. The manual annotation of corpora is a tedious and time consuming process. To obtain high-quality annotated data constitutes a bottleneck in machine learning for NLP today. Active learning is one way of easing the burden of annotation. This paper presents a first probe into the NLP research community concerning the nature of the annotation projects undertaken in general, and the use of active learning as annotation support in particular.\\n\\n    Title 2: Using Smaller Constituents Rather Than Sentences in Active Learning for {J}apanese Dependency Parsing\\n    Abstract 2: We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.\\n\\n    Title 3: Man vs. Machine: A Case Study in Base Noun Phrase Learning\\n    Abstract 3: A great deal of work has been done demonstrating the ability of machine learning algorithms to automatically extract linguistic knowledge from annotated corpora. Very little work has gone into quantifying the difference in ability at this task between a person and a machine. This paper is a first step in that direction.\\n\\nQuery: I am doing a research about active learning in NLP. Can you help me find some papers about it?\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[ \n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query_with_context,\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have found relevant information about your research topic in the following papers:\n",
      "- Title: Deep Reinforcement Learning for {NLP}\n",
      "  - Similarities: This paper directly addresses the application of deep reinforcement learning (DRL) to various NLP tasks, discussing practical solutions and recent advances in the field.\n",
      "  - Differences: While it covers foundational concepts and various applications, it may lack specific case studies detailing the performance metrics or comparison with traditional NLP methods.\n",
      "\n",
      "- Title: Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing\n",
      "  - Similarities: This paper applies reinforcement learning to a specific NLP task (dependency parsing), focusing on how it mitigates error propagation, which is a significant concern in NLP processes.\n",
      "  - Differences: It is narrower in scope, focusing specifically on greedy dependency parsing rather than a broader range of NLP applications or methods.\n",
      "\n",
      "- Title: Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks\n",
      "  - Similarities: This paper discusses the use of offline reinforcement learning in real-world NLP applications, which is a relevant aspect of reinforcement learning in the field, particularly concerning practical implementation challenges.\n",
      "  - Differences: It primarily focuses on offline learning and human feedback, offering a different perspective than the other papers, which emphasize more active learning and direct reinforcement scenarios.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_investigacion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
